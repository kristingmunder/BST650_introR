---
title: "Assignment 2"
author: "Kristin Gmunder"
date: "8/28/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Exercise Set 1

1. Create an object called "myAge" that holds your age *in months*. Hint: the
multiplication operator in R is "*".
```{r}
myAge <- 21*12+2
```
2. Using the object "myAge", approximate the number of days you've been alive. Use 30.44 days per month. Check this against your age in days on Wolfram Alpha. How many days off was your estimate?
```{r}
myAge*30.44
```
My estimate was exactly the same as in Wolfram Alpha.

3. Using the object "myAge", find your decimal age. Hint: the division operator is "/".
```{r}
(myAge*30.44)/365.25
```

4. Copy and Paste MyAge into the console and press ENTER. What happens? Why?

I got an output of "Error: object 'MyAge' not found," because R is case sensitive.  Therefore, R does not recognize the variable I was using (myAge) as the same as MyAge.


## Exercise Set 2

1. Five people start the newest diet craze. Their weights (in kg) before and after were [78 72 78 79 105] and [67 65 79 70 93], respectively. Store these two vectors as "wtBefore" and "wtAfter", respectively. Find the total weight lost, the average weight lost, and the weight lost per person. Hint: see the ?sum and ?mean functions.
```{R}
wtBefore <- c(78, 72, 78, 79, 105)
wtAfter <- c(67, 65, 79, 70, 93)
eachLost <- wtBefore-wtAfter
eachLost
totalLost <-sum(eachLost[1:5], na.rm = FALSE)
totalLost
avgWeight <- mean(eachLost)
avgWeight
```

2. Create the following vectors using the rep() and seq() functions:
    a) the first 30 non-negative integer powers of 2
```{R}
2^(0:10)
```    
    b) 1, 1.5, 2, 2.5, ..., 12
```{R}
seq(from = 1, to = 12, by = 0.5)
```
    c) 1, 8, 27, 64, ..., 1000
```{R}
(seq(from = 1, to = 10, by = 1))^3
```
    d) 1, -1/2, 1/3, -1/4, ..., -1/100
```{R}
(rep(c(1,-1), length.out = 100))/(seq(from = 1, to = 100, by = 1))
```
    e) 1, 0, 3, 0, 5, 0, 7, 0, ..., 0, 49
```{R}
(rep(c(1,0), length.out = 49))*(seq(from = 1, to = 49, by = 1))
```
    f) 1, 3, 6, 10, 15, ..., 210 (Hint: ?cumsum)
```{R}
cumsum(1:20)
```
    g) 1, 2, 2, 3, 3, 3, ..., 10, 10, 10, 10, 10, 10, 10, 10, 10, 10 (Hint: ?rep)
```{R}
rep(1:10, 1:10)
```

3. (ADVANCED) The jth term of the Taylor-MacLaurin series for the natural logarithm is   $$
\log(1 - x) := - \lim_{n \to \infty} \sum \limits_{j = 1}^n \frac{x ^ j}{j}.
  $$

Find the residuals of this approximation for n = 5, 10, 50, 100 around x = 0.25
```{R}
-1*sum((.25^(1:5))/(1:5))

-1*sum((.25^(1:10))/(1:10))

-1*sum((.25^(1:50))/(1:50))

-1*sum((.25^(1:100))/(1:100))
```




## Exercise Set 3

1. Create a vector of the months of the year and store it in an object called "mons".
```{R}
mons <- c("Jan", "Feb", "Mar", "Apr", "May", "June", "July", "Aug", "Sep", "Oct", "Nov", "Dec")
```
2. You would like to discard the first three months, so you type "mons[-1:3]". What happened? Why should you type instead?
When I typed "mons[-1:3]", I got an error code, "Error in mons[-1,3]: onlu 0's may be mixed with negative subscripts.  Instead, I should type:
```{R}
mons[-(1: 3)]
```

3. Find a method to use the "length()" function to find the last four entries of *any* atomic vector. Apply this to find the last four months of the year.
Do not understand the question
```{R}
lastFour <- mons[(length(mons)-3): length(mons)]
lastFour
```

4. Logically subset the uppercase vector of English letters in the following ways:
    a) Type LETTERS and press ENTER. What is your guess to find the lower case letters?

If I type in "letters", I get the lowercase letters.

    b) Create a vector of the first 12 upper-case English letters
```{R}
LETTERS[1:12]
```
    c) Create a vector of the 1st, 3rd, 5th, 7th, ..., 25th English letters
```{R}
LETTERS[seq(from = 1, to = 25, by = 2)]
```

    d) Create a vector of the consonants. (Hint: it may be easier to find the letters that are *not* one of the vowels)
```{R}
isVowel <- LETTERS %in% c("A", "E", "I", "O", "U")
LETTERS[!isVowel]
```   


</br>

*******************************************************************************
</br>


## Discrete Probability Distributions
### Negative Binomial Distribution
The Negative Binomial distribution models the number failures before we have the requred number of successes necessary to stop a process. For example, we may want to know how many interview candidates we will reject before we find a good fit, or how many patients we exclude before we have enough subjects in a clinical trial. That is, if we are looking to enroll 5 subjects according to a criteria, how many patient records will we skip before we find the 5 we're looking for? For this distribution, the probablility mass function is
\[
NB(x|n,p) := {x + n - 1 \choose n - 1} (1 - p) ^ x p ^ n.
\]

#### Estimate the Distribution Parameters Algebraically
From [Wikipedia](https://en.wikipedia.org/wiki/Negative_binomial_distribution), we have that the population mean of the negative binomial is $np/(1 - p)$, and the population variance is $np/(1 - p)^2$. I want to generate random samples from the
negative binomial distribution with a sample mean of 5 and sample variance of 10. Therefore, I have the following system of non-linear equations:
\[
\frac{np}{1 - p} = 5;\quad\ \frac{np}{(1 - p) ^ 2} = 10.
\]
After substitution, we have the following equation:
\[
\begin{align}
\frac{2np}{1 - p} &= \frac{np}{(1 - p) ^ 2} \\
\Longrightarrow\qquad 2(1 - p) ^ 2 &= 1 - p \qquad\ p \in (0,1) \\
\Longrightarrow\qquad\qquad\quad\ p &= 1/2.
\end{align}
\]
Substituting $p = 1/2$ into the equation for the mean yields $n = 5$. Therefore, the negative binomial distribution parameters that yield a population mean and variance of 5 and 10, respectively, are $n = 5$ and $p = 0.5$. Now, you may be saying "I don't want to do all that algebra!" In that case, you can always [ask WolframAlpha](https://www.wolframalpha.com/input/?i=n+*+p+%2F+(1+-+p)+%3D+5;+n+*+p+%2F+(1+-+p)+%5E+2+%3D+10) for help.

#### Generate Random Samples in `R`
Now that we have the parameters to input, we can generate random samples of size 5, 15, 25, and 1000, and calculate their sample means and variances. We know the true mean and variance is 5 and 10, respectively, so we will see how close the sample estimates are as the sample size increases. I will use the `set.seed()` function to make the random sample reproducible (but you don't have to set a seed for this assignment).
```{r n5}
set.seed(123)
# Sample Size of 5:
xNB_n5 <- rnbinom(n = 5, size = 5, prob = 0.5)
# If we enrolled 5 studies (the n = 5 argument), and were looking for 5 subjects
#   in each study (the size = 5 argument), with each subject independently
#   having a 50% chance of meeting the enrollment criteria (the prob = 0.5
#   argument), we would reject
xNB_n5
#   subjects from each trial before we will have enrolled 5 subjects.

# The average number of people we rejected before we enrolled five subjects in
#   each trial is
mean(xNB_n5)
#   and the variability of the number of people we rejected is
var(xNB_n5)
```

For this random sample, we see that that mean is much larger than we expected (7.8 instead of 5), and the variance is much smaller than we expected (2.2 instead of 10). In fact, if we performed a Student's $t$-test on these five observations, with corresponding significance of $p = 0.0135$, we would reject the claim that the population mean is 5! This is the danger of making statistical decisions using data sets with very small sample sizes (and from a skewed distribution).

Now, we will increase the sample size from 5 to 15, 25, and 1000.
```{r increaseN}
# Sample Size of 15
xNB_n15 <- rnbinom(n = 15, size = 5, prob = 0.5)
xNB_n15
mean(xNB_n15)
var(xNB_n15)


# Sample Size of 25
xNB_n25 <- rnbinom(n = 25, size = 5, prob = 0.5)
mean(xNB_n25)
var(xNB_n25)


# Sample Size of 100
xNB_n1000 <- rnbinom(n = 1000, size = 5, prob = 0.5)
mean(xNB_n1000)
var(xNB_n1000)
```


### Binomial Distribution
The probability mass function for the Binomial Distribution is 
\[
Bin(x|n,p) := {n \choose x} p ^ x (1 - p) ^ {n - x},
\]
where the population mean is $np$ and variance is $np(1 - p)$. Repeat the above process I applied to the Negative Binomial distribution, but with a population mean of 6 and variance of 2. Generate samples of sizes 5, 15, 25, and 1000 from such a distribution, and calculate their sample means and variances. 

I calculated: n = 9, p = 2/3

```{r}
set.seed(123)
# Sample Size of 5:
xB_n5 <- rbinom(n = 5, size = 9, prob = (2/3))

xB_n5

mean(xB_n5)

var(xB_n5)
```

For this random sample with n=5, we see that that mean is slightly smaller than we expected (5.8 instead of 6), and the variance is a bit larger than we expected (4.7 instead of 2). 

Now, we will increase the sample size from 5 to 15, 25, and 1000.
```{r}
# Sample Size of 15
xB_n15 <- rbinom(n = 15, size = 9, prob = (2/3))
xB_n15
mean(xB_n15)
var(xB_n15)

#### For this random sample with n=15, we see that that mean is very slightly larger than we expected (6.06667 instead of 6), and the variance is larger than we expected (3.066667 instead of 2).  As we increase n, the sample means and variances get closer to our population means and variances.

# Sample Size of 25
xB_n25 <- rbinom(n = 25, size = 9, prob = (2/3))
mean(xB_n25)
var(xB_n25)

#### For this random sample with n=25, we see that that mean is very slightly larger than we expected (6.04 instead of 6), and the variance is slightly smaller than we expected (1.79 instead of 2).  As we increase n, the sample means and variances get closer to our population means and variances.

# Sample Size of 1000
xB_n1000 <- rbinom(n = 1000, size = 9, prob = (2/3))
mean(xB_n1000)
var(xB_n1000)

#### For this random sample with n=1000, we see that that mean is very slightly smaller than we expected (5.946 instead of 6), and the variance is slightly smaller than we expected (1.890975 instead of 2).  As we increase n, the sample means and variances get closer to our population means and variances.
```


### Poisson Distribution
The probability mass function for the Poisson Distribution is
\[
Pois(x|\lambda) := \frac{\lambda ^ x e ^ {\lambda}}{x!},
\]
where the population mean and variance are both equal to $\lambda$. Repeat the process I applied to the Negative Binomial distribution, but with a population mean of 3 (and variance of 3). Generate samples of sizes 5, 15, 25, and 1000 from such a distribution, and calculate their sample means and variances. 

$\lambda$ = 3

```{r}
set.seed(123)
# Sample Size of 5:
xP_n5 <- rpois(n = 5, lambda = 3)

xP_n5

mean(xP_n5)

var(xP_n5)
```

For this random sample with n=5, we see that that mean is slightly smaller than we expected (2.2 instead of 3), and the variance is a bit smaller than we expected (1.2 instead of 3). 

Now, we will increase the sample size from 5 to 15, 25, and 1000.
```{r}
# Sample Size of 15
xP_n15 <- rpois(n = 15, lambda = 3)

xP_n15

mean(xP_n15)

var(xP_n15)

#### For this random sample with n=15, we see that that mean is very slightly larger than we expected (3.666667 instead of 3), and the variance is larger than we expected (4.666667 instead of 3).  As we increase n, the sample means and variances typically get closer to our population means and variances.  However, at n=15, the sample variance is still quite far off from our population variance.

# Sample Size of 25
xP_n25 <- rpois(n = 25, lambda = 3)
mean(xP_n25)
var(xP_n25)

#### For this random sample with n=25, we see that that mean is very slightly larger than we expected (3.12 instead of 3), and the variance is slightly smaller than we expected (2.69 instead of 3).  As we increase n, the sample means and variances get closer to our population means and variances.

# Sample Size of 1000
xP_n1000 <- rpois(n = 1000, lambda = 3)
mean(xP_n1000)
var(xP_n1000)

#### For this random sample with n=1000, we see that that mean is very slightly smaller than we expected (3.054 instead of 3), and the variance is slightly smaller than we expected (2.970054 instead of 3).  As we increase n, the sample means and variances get closer to our population means and variances.
```

</br>

*******************************************************************************
</br>


## Continuous Distributions
There are many common continuous statistical distributions, but we will cover the four most used in statistical inference: the [Normal Distribution](https://en.wikipedia.org/wiki/Normal_distribution), [Student's $t$ Distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution), [Chi Square Distribution](https://en.wikipedia.org/wiki/Chi-squared_distribution), and the [$F$-Distribution](https://en.wikipedia.org/wiki/F-distribution).


### Normal Distribution
The probability density function for the Normal Distribution is
\[
N(x|\mu, \sigma) := \frac{1}{\sqrt{2 \pi \sigma ^ 2}} e ^ {-\left(\frac{(x - \mu) ^ 2}{2 \sigma ^ 2}\right)},
\]
with mean $\mu$ and variance $\sigma ^ 2$. Repeat the process I applied to the Negative Binomial distribution, but with a population mean of 2 and variance of 5. Generate samples of sizes 5, 15, 25, and 1000 from such a distribution, and calculate their sample means and variances. 

$\mu$ = 2
$\sigma$ = sqrt(5)

```{r}
set.seed(123)
# Sample Size of 5:
xN_n5 <- rnorm(n = 5, mean = 2, sd = sqrt(5))

xN_n5

mean(xN_n5)

var(xN_n5)
```

For this random sample with n=5, we see that that mean is slightly smaller than we expected (1.751816 instead of 2), and the variance is much larger than we expected (9.042284 instead of 5). 

Now, we will increase the sample size from 5 to 15, 25, and 1000.
```{r}
# Sample Size of 15
xN_n15 <- rnorm(n = 15, mean = 2, sd = sqrt(5))

xN_n15

mean(xN_n15)

var(xN_n15)

#### For this random sample with n=15, we see that that mean is very slightly larger than we expected (2.285285 instead of 2), and the variance is smaller than we expected (4.133062 instead of 5).  As we increase n, the sample means and variances get closer to our population means and variances.  

# Sample Size of 25
xN_n25 <- rnorm(n = 25, mean = 2, sd = sqrt(5))
mean(xN_n25)
var(xN_n25)

#### For this random sample with n=25, we see that that mean is slightly larger than we expected (2.296206 instead of 2), and the variance is larger than we expected (5.616601 instead of 5).  As we increase n, the sample means and variances get closer to our population means and variances.

# Sample Size of 1000
xN_n1000 <- rnorm(n = 1000, mean = 2, sd = sqrt(5))
mean(xN_n1000)
var(xN_n1000)

#### For this random sample with n=1000, we see that that mean is very slightly smaller than we expected (1.974994 instead of 2), and the variance is slightly larger than we expected (5.106231 instead of 5).  As we increase n, the sample means and variances get closer to our population means and variances.
```

### Student's $t$ Distribution
The probability density function for the Student's $t$ Distribution is
\[
t_{\nu}(x) := \frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\sqrt{\nu \pi} \Gamma\left(\frac{\nu}{2}\right)} \left(1 + \frac{x ^ 2}{\nu}\right) ^ {-\left(\frac{\nu + 1}{2}\right)},
\]
with mean identically $0$ and variance $\nu / (\nu - 2)$. This and other distributions make use of the $\Gamma$ function (read: "Gamma"); this function is defined as
\[
\Gamma(n) := (n - 1)!,\ n \in \mathbb{N},
\]
where $\mathbb{N}$ is the set of counting numbers and $!$ is the [factorial operator](https://en.wikipedia.org/wiki/Factorial) (the $\Gamma$ function is technically defined for all complex numbers with a positive real component, but that's a conversation for your mathematical statistics class). Repeat the process I applied to the Negative Binomial distribution, but with a population variance of 2. Generate samples of sizes 5, 15, 25, and 1000 from such a distribution, and calculate their sample means and variances. 

I calculated: v (df) = 4

```{r}
set.seed(123)
# Sample Size of 5:
xT_n5 <- rt(n = 5, df = 4)

xT_n5

mean(xT_n5)

var(xT_n5)
```

For this random sample with n=5, we see that that mean is slightly smaller than we expected (-0.4666473 instead of 0), and the variance is a bit smaller than we expected (0.8654073 instead of 2). 

Now, we will increase the sample size from 5 to 15, 25, and 1000.
```{r}
# Sample Size of 15
xT_n15 <- rt(n = 15, df = 4)
xT_n15
mean(xT_n15)
var(xT_n15)

#### For this random sample with n=15, we see that that mean is very slightly smaller than we expected (-0.02957297 instead of 0), and the variance is smaller than we expected (0.8797166 instead of 2).  As we increase n, the sample means and variances get closer to our population means and variances.

# Sample Size of 25
xT_n25 <- rt(n = 25, df = 4)
mean(xT_n25)
var(xT_n25)

#### For this random sample with n=25, we see that that mean is very slightly larger than we expected (0.1398457 instead of 0), and the variance is slightly smaller than we expected (3.566582 instead of 2).  As we increase n, the sample means and variances typically get closer to our population means and variances.  However, at n=25, the sample variance is still quite far off from our population variance.

# Sample Size of 1000
xT_n1000 <- rt(n = 1000, df = 4)
mean(xT_n1000)
var(xT_n1000)

#### For this random sample with n=1000, we see that that mean is very slightly larger than we expected (0.05973646 instead of 0), and the variance is slightly larger than we expected (2.539913 instead of 2).  As we increase n, the sample means and variances get closer to our population means and variances.
```

### $\chi ^ 2$ Distribution
The probability density function for the $\chi ^ 2$ Distribution (also typeset as the Chi-Squared Distribution) is
\[
\chi ^ 2 _{\nu}(x) := \frac{2 ^ {-\frac{\nu}{2}}}{\Gamma\left(\frac{\nu}{2}\right)} e ^ {-\frac{x}{2}} x ^ {\frac{\nu}{2} - 1},
\]
with mean $\nu$ and variance $2 \nu$. Repeat the process I applied to the Negative Binomial distribution, but with a population mean of 2 (and thus a variance of 4). Generate samples of sizes 5, 15, 25, and 1000 from such a distribution, and calculate their sample means and variances. 

I calculated: v (df) = 2

```{r}
set.seed(123)
# Sample Size of 5:
xC_n5 <- rchisq(n = 5, df = 2)

xC_n5

mean(xC_n5)

var(xC_n5)
```

For this random sample with n=5, we see that that mean is a bit larger than we expected (3.122728 instead of 2), and the variance is a bit larger than we expected (5.145684 instead of 4). 

Now, we will increase the sample size from 5 to 15, 25, and 1000.
```{r}
# Sample Size of 15
xC_n15 <- rchisq(n = 15, df = 2)
mean(xC_n15)
var(xC_n15)

#### For this random sample with n=15, we see that that mean is slightly larger than we expected (2.248956 instead of 2), and the variance is much smaller than we expected (1.497102 instead of 4).  As we increase n, the sample means and variances tend to get closer to our population means and variances.

# Sample Size of 25
xC_n25 <- rchisq(n = 25, df = 2)
mean(xC_n25)
var(xC_n25)

#### For this random sample with n=25, we see that that mean is slightly larger than we expected (2.24683 instead of 2), and the variance is much larger than we expected (7.054499 instead of 4).  As we increase n, the sample means and variances typically get closer to our population means and variances.  However, at n=25, the sample variance is still quite far off from our population variance.

# Sample Size of 1000
xC_n1000 <- rchisq(n = 1000, df = 2)
mean(xC_n1000)
var(xC_n1000)

#### For this random sample with n=1000, we see that that mean is very slightly smaller than we expected (1.997814 instead of 2), and the variance is slightly smaller than we expected (3.925057 instead of 4).  As we increase n, the sample means and variances get closer to our population means and variances.
```

### $F$-Distribution
The probability density function for the $F$-Distribution is
\[
F(x|n,m) := \frac{\Gamma\left(\frac{n + m}{2}\right) \sqrt{n ^ n m ^ m}}{\Gamma\left(\frac{n}{2}\right) \Gamma\left(\frac{m}{2}\right)} \sqrt{\frac{x ^ {n - 2}}{(nx + m) ^ {n + m}}},
\]
with mean $m / (m - 2)$ and variance
\[
\frac{2m ^ 2 (n + m - 2)}{n (m - 2) ^ 2 (m - 4)},\ m > 4.
\]
Repeat the process I applied to the Negative Binomial distribution, but with a population mean of 1.2 and variance of 0.4. Generate samples of sizes 5, 15, 25, and 1000 from such a distribution, and calculate their sample means and variances. 

I calculated: m (df2) = 12 and n (df1) = 90

```{r}
set.seed(123)
# Sample Size of 5:
xF_n5 <- rf(n = 5, df1 = 90, df2 = 12)
xF_n5
mean(xF_n5)
var(xF_n5)
```

For this random sample with n=5, we see that that mean is slightly larger than we expected (1.459374 instead of 1.2), and the variance is slightly larger than we expected (0.4466159 instead of 0.4). 

Now, we will increase the sample size from 5 to 15, 25, and 1000.
```{r}
# Sample Size of 15
xF_n15 <- rf(n = 15, df1 = 90, df2 = 12)
mean(xF_n15)
var(xF_n15)

#### For this random sample with n=15, we see that that mean is slightly smaller than we expected (0.9343352 instead of 1.2), and the variance is slightly smaller than we expected (0.2142377 instead of 0.4).  As we increase n, the sample means and variances tend to get closer to our population means and variances.

# Sample Size of 25
xF_n25 <- rf(n = 25, df1 = 90, df2 = 12)
mean(xF_n25)
var(xF_n25)

#### For this random sample with n=25, we see that that mean is slightly smaller than we expected (1.084133 instead of 1.2), and the variance is slightly smaller than we expected (0.2630517 instead of 0.4).  As we increase n, the sample means and variances get closer to our population means and variances.  

# Sample Size of 1000
xF_n1000 <- rf(n = 1000, df1 = 90, df2 = 12)
mean(xF_n1000)
var(xF_n1000)

#### For this random sample with n=1000, we see that that mean is very close to what we expected (1.203602 instead of 1.2), and the variance is slightly smaller than we expected (0.3639529 instead of 0.4).  As we increase n, the sample means and variances get closer to our population means and variances.
```
